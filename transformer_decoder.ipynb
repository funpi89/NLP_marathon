{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funpi89/NLP_marathon/blob/main/transformer_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu1c8eWEQG76"
      },
      "source": [
        "#Transformer decoder\n",
        "***\n",
        "- 實做 Transformer decoder 以更了解　Transformer \n",
        "- 應用 Transformer decoder 建立一個簡單的 ptt 貼文回應器 驗證 Transformer decoder 可以運行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZljCWHhScnI"
      },
      "source": [
        "# [教學目標]\n",
        "- 了解如何實作 transformer decoder 和其結構\n",
        "- 了解如何應用 transformer decoder 並證明 decoder 可以作用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C50wfDUxTFvZ"
      },
      "source": [
        "# [重點]\n",
        "- 觀察 TransformerDecoder 的建立\n",
        "- 觀察 TransformerDecoderLayer 的建立\n",
        "-- 使用 encoder 相同的 MultiHeadAttentionSubLayer\n",
        "-- 使用 encoder 相同的 PosFeedForwardSubLayer\n",
        "- 觀察如何使用 建立的 TransformerDecoder \n",
        "-- 使用 TransformerDecoder 做序列生成 SequenceGenerate\n",
        "-- 如何使用 SequenceGenerate 模型 訓練一個 ptt 回應機"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNsXiF46TvpP"
      },
      "source": [
        "# [結構]\n",
        "- TransformerDecoder 模型和 SequenceGenerate 實作\n",
        "- ptt 資料準備\n",
        "- 應用 SequenceGenerate 訓練 ptt answer machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVOgcAidH989"
      },
      "source": [
        "# import 需要的 packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT2vUzUAXkik"
      },
      "source": [
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOKbhccjXtg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f59eb0-9367-43a8-9a5c-f800d877733b"
      },
      "source": [
        "# 連接個人資料 讀取 ＰＴＴ 訓練資料和儲存模型\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOuTtZbWhIj"
      },
      "source": [
        "# 實做 TransformerDecoder\n",
        "- 如果只用 Transfomer decoder 而已 不和 encoder　一起使用 \n",
        "-- skip_encoder_attn 不需要和 encoder attention\n",
        "-- enc_hidden　和 enc_mask　不用輸入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiWfhvskI2fH"
      },
      "source": [
        "## 啟動參數\n",
        "## hidden_dim 內部 embedding 大小\n",
        "## feedforward_dim  feedforward 中間層大小\n",
        "## n_dec_layers 幾層 Transformer Layers\n",
        "## n_attn_heads 幾個 attention heads \n",
        "## dropout dropout 比例\n",
        "## dec_voca_length  字彙集合大小\n",
        "## max_pos_length  最大 decode 序列長度\n",
        "## device \n",
        "## skip_encoder_attn 不需要和 encoder attention\n",
        "\n",
        "## 輸入值\n",
        "## dec_seq 解碼序列　（句子）\n",
        "## enc_hidden　編碼的　hidden embedding　(Optional)\n",
        "## dec_mask 解碼遮罩\n",
        "## enc_mask　編碼遮罩　(Optional)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, hidden_dim, feedforward_dim, n_dec_layers, n_attn_heads, dropout, dec_voca_length, max_pos_length , device , skip_encoder_attn = False):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    # 建立 decoder token embedding \n",
        "    self.dec_tok_embedding = nn.Embedding(dec_voca_length, hidden_dim )\n",
        "    # 建立 decoder position embedding \n",
        "    self.dec_pos_embedding = nn.Embedding(max_pos_length, hidden_dim)\n",
        "\n",
        "    # 建立 n_dec_layers 個 TransformerDecoderLayer 層\n",
        "    self.transformer_decoder_layers = nn.ModuleList([TransformerDecoderLayer(hidden_dim,\n",
        "                                          feedforward_dim, \n",
        "                                          n_dec_layers,\n",
        "                                          n_attn_heads,\n",
        "                                          dropout, \n",
        "                                          device, skip_encoder_attn) for _ in range(n_dec_layers)])\n",
        "\n",
        "    # 輸出層 輸出 vocabulary 個長度\n",
        "    self.full_conn_out = nn.Linear(hidden_dim, dec_voca_length)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "  def forward(self, dec_seq, enc_hidden , dec_mask, enc_mask):\n",
        "    #dec_seq 輸入 tensor 形狀 [batch size, decode sequence len]\n",
        "    #enc_hidden 輸入 tensor 形狀 [batch size, encode sequence len, hid dim] # optional 不需要時輸入空值\n",
        "    #dec_mask 輸入 tensor 形狀 [batch size, decode sequence len]\n",
        "    #enc_mask 輸入 tensor 形狀 [batch size, encode sequence len] # optional 不需要時輸入空值\n",
        "                \n",
        "    batch_size = dec_seq.shape[0]\n",
        "    dec_len = dec_seq.shape[1]\n",
        "        \n",
        "    pos = torch.arange(0, dec_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "    #pos 的 tensor 形狀 [batch size, decode sequence len]\n",
        "            \n",
        "    # 將 decoder token embedding 加上 decoder postion embedding\n",
        "    dec_seq = self.dropout(self.dec_tok_embedding(dec_seq)  + self.dec_pos_embedding(pos))\n",
        "                \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "        \n",
        "    for layer in self.transformer_decoder_layers:\n",
        "      dec_seq, encoder_decoder_attention , decoder_self_attention = layer(dec_seq, enc_hidden, dec_mask, enc_mask)\n",
        "        \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence  len, hid dim]\n",
        "    #attention 輸出 tensor 形狀 [batch size, n heads, trg len, src len]\n",
        "        \n",
        "    output = self.full_conn_out(dec_seq)\n",
        "        \n",
        "    #output tensor 形狀 [batch size, trg len, output dim]\n",
        "            \n",
        "    return output, encoder_decoder_attention , decoder_self_attention\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNvw02zeMGIT"
      },
      "source": [
        "# 實做 TransformerDecoderLayer\n",
        "- 實作在transformerDecoder 使用多層 的TransformerDecoderLayer\n",
        "- 如果只使用 decoder 則不用 encoder attention, --> skip_encoder_attn = True "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAV9O7SnJsLU"
      },
      "source": [
        "## 啟動參數\n",
        "## hidden_dim 內部 embedding 大小\n",
        "## feedforward_dim  feedforward 中間層大小\n",
        "## n_dec_layers 幾層 Transformer Layers\n",
        "## n_attn_heads 幾個 attention heads \n",
        "## dropout dropout 比例\n",
        "## device \n",
        "## skip_encoder_attn 不需要和 encoder attention\n",
        "\n",
        "## 輸入值\n",
        "## dec_seq 解碼序列　（句子）\n",
        "## enc_hidden　編碼的　hidden embedding　(Optional)\n",
        "## dec_mask 解碼遮罩\n",
        "## enc_mask　編碼遮罩　(Optional)\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim , feedforward_dim, n_dec_layers, n_attn_heads, dropout , device , skip_encoder_attn = False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.skip_encoder_attn = skip_encoder_attn \n",
        "\n",
        "    self.self_attention_sublayer = MultiHeadAttentionSubLayer(hidden_dim, n_attn_heads, dropout, device)\n",
        "    self.self_attn_layernorm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    if not skip_encoder_attn:\n",
        "      self.encoder_attention_sublayer = MultiHeadAttentionSubLayer(hidden_dim, n_attn_heads, dropout, device)\n",
        "      self.encoder_attn_layernorm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    self.positionwise_feedforward = PosFeedForwardSubLayer(hidden_dim,feedforward_dim ,dropout)\n",
        "    self.feedforward_layernorm = nn.LayerNorm(hidden_dim)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)     \n",
        "\n",
        "  def forward(self, dec_seq, enc_hidden , dec_mask, enc_mask):\n",
        "    #dec_seq 輸入 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "    #enc_hidden 輸入 tensor 形狀 [batch size, encode sequence len, hid dim] # optional 不需要時輸入空值\n",
        "    #dec_mask 輸入 tensor 形狀 [batch size, decode sequence len]\n",
        "    #enc_mask 輸入 tensor 形狀 [batch size, encode sequence len] # optional 不需要時輸入空值\n",
        "        \n",
        "    #self attention 子層\n",
        "    _dec_seq, decoder_self_attention = self.self_attention_sublayer(dec_seq, dec_seq, dec_seq, dec_mask)\n",
        "        \n",
        "    #dropout, residual connection and layer norm　(Add and Norm)\n",
        "    dec_seq = self.self_attn_layernorm(dec_seq + self.dropout(_dec_seq))\n",
        "            \n",
        "    #dec_seq  輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "            \n",
        "    # 需不需要建立　encoder attention 層        \n",
        "    if not self.skip_encoder_attn:\n",
        "      #encoder attention\n",
        "      _dec_seq, encoder_decoder_attention = self.encoder_attention_sublayer(dec_seq, enc_hidden, enc_hidden, enc_mask)\n",
        "          \n",
        "      #dropout, residual connection and layer norm\n",
        "      dec_seq = self.encoder_attn_layernorm(dec_seq + self.dropout(_dec_seq))\n",
        "    else:\n",
        "      encoder_decoder_attention = None\n",
        "                    \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "    #positionwise feedforward\n",
        "    _dec_seq = self.positionwise_feedforward(dec_seq)\n",
        "        \n",
        "    #dropout, residual and layer norm (Add and Norm)\n",
        "    dec_seq = self.feedforward_layernorm(dec_seq + self.dropout(_dec_seq))\n",
        "        \n",
        "    #dec_seq 輸出 tensor 形狀 [batch size, decode sequence len, hid dim]\n",
        "    #attention 輸出 tensor 形狀 [batch size, n heads, decode sequence len, encode sequence len]\n",
        "        \n",
        "    return dec_seq, encoder_decoder_attention , decoder_self_attention\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MZZrOaLOEQn"
      },
      "source": [
        "# 實做 MultiHeadAttentionSubLayer\n",
        "- 實作 encoder and decoder 同時共用的 MultiHeadAttention SubLayer \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_9DTM9jJyDR"
      },
      "source": [
        "## 啟動參數\n",
        "## hidden_dim 內部 embedding 大小\n",
        "## n_attn_heads 幾個 attention heads \n",
        "## dropout dropout 比例\n",
        "## device \n",
        "\n",
        "## 輸入值\n",
        "## query_input, --> K \n",
        "## key_input, --> Q\n",
        "## value_input, --> V\n",
        "## mask 遮罩\n",
        "\n",
        "class MultiHeadAttentionSubLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim , n_attn_heads, dropout, device):\n",
        "    super().__init__()\n",
        "\n",
        "    # 確定 設定的 hidden layer 維度可以被 attention head 整除\n",
        "    assert hidden_dim % n_attn_heads ==0\n",
        "\n",
        "    # hidden layer 維度\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # multi-heads 的個數\n",
        "    self.n_attn_heads = n_attn_heads\n",
        "\n",
        "    # 平均分到每個 multi-head 的 維度\n",
        "    self.head_dim = hidden_dim // n_attn_heads\n",
        "\n",
        "    # 就是在課程中提到的 Wq Wk Wv\n",
        "    self.full_conn_q = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.full_conn_k = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.full_conn_v = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    # 最後結果再過一層 線性轉換\n",
        "    self.full_conn_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    # 根據維度大小調整 attention 值 以免維度太大 Q dot K 結果過大影響學習效率    \n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  \n",
        "\n",
        "  def forward(self, query_input, key_input, value_input, mask = None):\n",
        "    batch_size = query_input.shape[0]\n",
        "\n",
        "    #query_input shape [batch size, query len, hid dim]\n",
        "    #key_input shape [batch size, key len, hid dim]\n",
        "    #value_input shape [batch size, value len, hid dim]\n",
        "\n",
        "    Q = self.full_conn_q(query_input)\n",
        "    K = self.full_conn_k(key_input)\n",
        "    V = self.full_conn_v(value_input)\n",
        "\n",
        "    #Q shape [batch size, query len, hid dim]\n",
        "    #K shape [batch size, key len, hid dim]\n",
        "    #V shape [batch size, value len, hid dim]\n",
        "\n",
        "    # 將 attention 切成多塊小的 attention\n",
        "    def split_attention(Q, K, V):\n",
        "      Q = Q.view(batch_size, -1, self.n_attn_heads, self.head_dim)\n",
        "      K = K.view(batch_size, -1, self.n_attn_heads, self.head_dim)\n",
        "      V = V.view(batch_size, -1, self.n_attn_heads, self.head_dim)\n",
        "      return Q , K , V\n",
        "\n",
        "    # 將 attention 的 2 和 3 維度轉置 以達到將 attention head 提到前面 而分開每個 attention head\n",
        "    def seperate_heads(Q, K, V):\n",
        "      Q = Q.permute(0, 2, 1, 3) # (batch_size, self.n_heads , query len , self.head_dim)\n",
        "      K = K.permute(0, 2, 1, 3) # (batch_size, self.n_heads , key len , self.head_dim)\n",
        "      V = V.permute(0, 2, 1, 3) # (batch_size, self.n_heads , value len , self.head_dim)\n",
        "      return Q , K , V\n",
        "\n",
        "    Q, K, V = split_attention(Q, K, V)\n",
        "\n",
        "    Q, K, V = seperate_heads (Q, K, V)\n",
        "\n",
        "    \n",
        "    # 調整過的 dot product attention, 由於之前分開了每個 attention head \n",
        "    # 所以現在只要把 Ｋ的最後兩個維度轉置 就可以 by attention head 求得 Q dot K\n",
        "    scaled_dot_product_similarity = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    #scaled_dot_product_similarity 輸出 [batch size, n heads, query len, key len]\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_dot_product_similarity = scaled_dot_product_similarity.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product_similarity, dim = -1)\n",
        "    #attention = [batch size, n heads, query len, key len]\n",
        "\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "    #x 輸出 [batch size, n heads, query len, head dim]\n",
        "        \n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "    #x 輸出 [batch size, query len, n heads, head dim]\n",
        "        \n",
        "    x = x.view(batch_size, -1, self.hidden_dim)\n",
        "        \n",
        "    #x 輸出 [batch size, query len, hid dim]\n",
        "        \n",
        "    x = self.full_conn_o(x)\n",
        "        \n",
        "    #x 輸出 [batch size, query len, hid dim]\n",
        "        \n",
        "    return x, attention"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kg56sgOzpj"
      },
      "source": [
        "# 實做 PosFeedForwardSubLayer\n",
        "- 實作 encoder and decoder 同時共用的 PosFeedForward SubLayer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv_5iQiFJ37I"
      },
      "source": [
        "class PosFeedForwardSubLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, ff_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.full_conn_1 = nn.Linear(hidden_dim, ff_dim)\n",
        "    self.full_conn_2 = nn.Linear(ff_dim,  hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "    x = self.dropout(torch.relu(self.full_conn_1(x)))\n",
        "        \n",
        "    #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "    x = self.full_conn_2(x)\n",
        "        \n",
        "    #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "    return x\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Awfn3ELO8sg"
      },
      "source": [
        "# 實做 SequenceGenerate \n",
        "- 處理 序列生成工作\n",
        "- 叫用 TransformerDecoderLayer\n",
        "-- 不使用 encoder decoder attention 子層\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URWQcxLJJ63p"
      },
      "source": [
        "## 啟動參數\n",
        "## decoder　Transformer decoder\n",
        "## dec_pad_idx decoder padding index  \n",
        "## device \n",
        "\n",
        "## 輸入值\n",
        "## dec_seq 解碼訓練\n",
        "class SequenceGenerate(nn.Module):\n",
        "  def __init__(self, decoder, dec_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.decoder = decoder\n",
        "    self.dec_pad_idx = dec_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "  def make_dec_mask(self, dec_seq):\n",
        "        \n",
        "    #dec_seq 輸入 [batch size, decoder sequence len]\n",
        "        \n",
        "    dec_pad_mask = (dec_seq != self.dec_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "    #dec_pad_mask 輸出 [batch size, 1, 1, decoder sequence len]\n",
        "        \n",
        "    dec_len = dec_seq.shape[1]\n",
        "        \n",
        "    dec_sub_mask = torch.tril(torch.ones((dec_len, dec_len), device = self.device)).bool()\n",
        "        \n",
        "    #dec_sub_mask 輸出 [decoder sequence len, decoder sequence len]\n",
        "            \n",
        "    dec_mask = dec_pad_mask & dec_sub_mask\n",
        "        \n",
        "    #dec_mask 輸出 [batch size, 1, decoder sequence len, decoder sequence len]\n",
        "        \n",
        "    return dec_mask\n",
        "\n",
        "  def forward(self, dec_seq):\n",
        "        \n",
        "    #dec_seq 輸入　tensor [batch size, trg len]\n",
        "                \n",
        "    dec_mask = self.make_dec_mask(dec_seq)\n",
        "        \n",
        "    #dec_mask 輸出 [batch size, 1, trg len, trg len]\n",
        "        \n",
        "    # 呼叫　transformer decoder 不需要輸入　encoder 相關資訊\n",
        "    # 也不用接收　encoder decoder attnetion            \n",
        "    output, _ , decoder_self_attention = self.decoder(dec_seq, None, dec_mask, None)\n",
        "        \n",
        "    #output 輸出 [batch size, trg len, output dim]\n",
        "    #attention 輸出 [batch size, n heads, trg len, src len]\n",
        "        \n",
        "    return output, decoder_self_attention"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM76XyjFSw-m"
      },
      "source": [
        "# PTT 資料準備\n",
        "\n",
        "- 我們的資料來源是 https://www.kaggle.com/zake7749/pttgossipingcorpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tr2kzh-L9kb",
        "outputId": "739f63cd-108e-4b25-de49-6560a4e6d135"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRBsuTjcSxhm",
        "outputId": "e83cecb1-01eb-49fd-cb4d-3b3b5e675ddb"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/NLPMarathon/'\n",
        "with open('/content/drive/MyDrive/NLPMarathon/Gossiping-QA-Dataset-2_0.csv' , encoding='utf-8') as fin:\n",
        "  csvreader = csv.reader(fin)\n",
        "  ptt_qa_pairs = [ row for row in csvreader]\n",
        "\n",
        "print (\"Sample: \" , ptt_qa_pairs[1000][0:2] )\n",
        "print (\"Total records:\" , len(ptt_qa_pairs))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['油價又要噴出了??', '政府：中油臺電內部控管不佳；財團：民營化砍肥貓']\n",
            "Total records: 774115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POdVnkF5XZRg"
      },
      "source": [
        "# do training test split 如果已經分過了 可以跳過這段"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDH9OthiSzqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7072d822-fc5b-46ec-8cd9-3653b62a15ba"
      },
      "source": [
        "\n",
        "print (\"Total records after filtering :\" , len(ptt_qa_pairs))\n",
        "train, val = train_test_split(ptt_qa_pairs, test_size=10000)\n",
        "\n",
        "print (\"training data:{} , develop data: {} \".format(len(train),len(val)))\n",
        "    \n",
        "def write_csv(trn_data, file_path ):\n",
        "    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n",
        "        writer = csv.writer (fout)\n",
        "        for itm in trn_data: \n",
        "            writer.writerow ([itm[0] + \"|\" + itm[1] , itm[0] + \"|\" + itm[1]] )\n",
        "            \n",
        "file_path = data_dir + 'train.csv'\n",
        "write_csv(train, file_path )\n",
        "\n",
        "file_path = data_dir + 'val.csv'\n",
        "write_csv(val, file_path )\n",
        "    \n",
        "#file_path = data_dir + 'test.csv'\n",
        "# write_csv(test, file_path )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records after filtering : 774115\n",
            "training data:764115 , develop data: 10000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lVcYLXjTDf4"
      },
      "source": [
        "# 資料處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAO08pxJTEMh"
      },
      "source": [
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = text.replace(\"\\\\\",\"\").split(\"|\")\n",
        "\n",
        "  return [word for word in regex.sub(text[0],' ') if word.strip()] + [\"<sep>\"] + [word for word in regex.sub(text[1],' ') if word.strip()]\n",
        "\n",
        "def tokenize_trg(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = text.replace(\"\\\\\",\"\").split(\"|\")\n",
        "\n",
        "  return ['<pad>' for word in regex.sub(text[0],' ') if word.strip()] + [\"<pad>\"] + [word for word in regex.sub(text[1],' ') if word.strip()]   \n",
        "\n",
        "CMN_FIELD = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG_FIELD = Field(tokenize = tokenize_trg, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "train_dataset, dev_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = False,\n",
        "    train='train.csv', validation='val.csv',\n",
        "    fields=[\n",
        "        ('qa', CMN_FIELD),\n",
        "        ('trg', TRG_FIELD)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rMbQIsMV0pF"
      },
      "source": [
        "# 我們要使用的資料格式\n",
        "- 建立 vocabulary\n",
        "- qa: ptt 上蒐集的問題和回答 中間用 “sep”隔開\n",
        "- trg: 我們的訓練目標只有回答的部分，其他的字元（包括“sep”）我們都以 “pad” 取代 , 計算 loss 的時候系統會忽略 ”pad“ token 註記的目標"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7NXC4GhTRDB",
        "outputId": "1763427b-f4e2-4d05-acac-18b75b479ad5"
      },
      "source": [
        "CMN_FIELD.build_vocab(train_dataset, min_freq = 2)\n",
        "TRG_FIELD.vocab = CMN_FIELD.vocab\n",
        "print (\"中文語料的字元表長度: \" , len(CMN_FIELD.vocab) )\n",
        "print (\"Sample Q and A:\", dev_dataset[0].qa)\n",
        "print (\"Sample Target:\",  dev_dataset[0].trg  )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  6531\n",
            "Sample Q and A: ['朱', '自', '清', '憑', '什', '麼', '入', '選', '課', '本', '的', '八', '卦', '<sep>', '純', '正', '中', '國', '人', '血', '統', '啊']\n",
            "Sample Target: ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '純', '正', '中', '國', '人', '血', '統', '啊']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydwZItmV2KF"
      },
      "source": [
        "# 準備 train_iterator and valid_iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldRVLGBJTXiB"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.qa),\n",
        "     device = device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTfx3TuR-5Z"
      },
      "source": [
        "# model training and evaluate function\n",
        "- 注意 我們要輸入的字和目標要shift 一位 \n",
        "- 也就是輸入 為', '什', '麼', '淘', '寶', '一', '堆', '賣', '家', '能', '國', '內', '免', '運', '?', '<sep>' --> 希望輸出 '有'\n",
        "- 輸入 為', '什', '麼', '淘', '寶', '一', '堆', '賣', '家', '能', '國', '內', '免', '運', '?', '<sep>', '有' --> 希望輸出 '的'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIcei9Ooblb9"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        qa = batch.qa\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _  = model(qa[:,:-1])\n",
        "                \n",
        "        # print (output.shape, trg.shape)\n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if i % 1000 == 0: print (\"Train Batch:\" , i , \"Loss:\" , loss.item())\n",
        "        \n",
        "    \n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AobBpcaSboe8"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            qa = batch.qa\n",
        "            trg = batch.trg\n",
        "\n",
        "            \n",
        "            output, _  = model(qa[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asQT783VaaUU"
      },
      "source": [
        "# 實際建立模型\n",
        "- 設定重要參數\n",
        "-- 建立一個 hidden embedding 256，三層decoder layer，八個attention heads\n",
        "-- position wise feedforward 中間層 512 dropout 0.1 learning rate: 0.0005\n",
        "-- 最長句長 70\n",
        "- 如果要保留訓練出來的模型，建議和 vocabulary 一起儲存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtWL65_Ha1dX"
      },
      "source": [
        "model_dir =  '/content/drive/MyDrive/NLPMarathon/transformer/model/'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "VOC_SIZE = len(CMN_FIELD.vocab)\n",
        "MAX_SENT_LENGTH = 70\n",
        "HID_DIM = 256\n",
        "DEC_LAYERS = 3\n",
        "DEC_HEADS = 8\n",
        "DEC_FF_DIM = 512\n",
        "DEC_DROPOUT = 0.1\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "dec = TransformerDecoder(HID_DIM, DEC_FF_DIM,\n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS,  \n",
        "              DEC_DROPOUT, \n",
        "              VOC_SIZE, MAX_SENT_LENGTH,\n",
        "              device , skip_encoder_attn = True)\n",
        "\n",
        "CMN_PAD_IDX = CMN_FIELD.vocab.stoi[CMN_FIELD.pad_token]\n",
        "\n",
        "#TransformerSequenceGenerate\n",
        "model = SequenceGenerate(dec, CMN_PAD_IDX, device).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = CMN_PAD_IDX)\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDXajaXalpKX"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOnClWtxc8VI"
      },
      "source": [
        "# 實際訓練\n",
        "- Ｔ4 大約 四分半一個 epoch\n",
        "- 訓練十個 epoch 就有一定的成績了\n",
        "- 如果沒時間訓練 也可以下載我們訓練好的權重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzmG1zJ4a6_v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af70361f-c122-4552-a6e3-fd11e2ffba40"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = 9999999\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    #torch.save(model.state_dict(), model_dir + 'model-ptt-{}.pt'.format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), model_dir + 'model-ptt-best.pt')\n",
        "\n",
        "    \n",
        "    print (\"Epoch {} training time: {:.2f} sec Training Loss: {:.3f} , Valiation Loss: {:.3f}\".format( epoch , end_time - start_time , train_loss , valid_loss))\n",
        " \n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Batch: 0 Loss: 8.837539672851562\n",
            "Train Batch: 1000 Loss: 4.960432052612305\n",
            "Train Batch: 2000 Loss: 4.7194318771362305\n",
            "Train Batch: 3000 Loss: 4.694741725921631\n",
            "Train Batch: 4000 Loss: 4.557633399963379\n",
            "Train Batch: 5000 Loss: 4.376219749450684\n",
            "Epoch 0 training time: 239.06 sec Training Loss: 4.693 , Valiation Loss: 4.289\n",
            "Train Batch: 0 Loss: 4.227318286895752\n",
            "Train Batch: 1000 Loss: 4.355353355407715\n",
            "Train Batch: 2000 Loss: 4.327803134918213\n",
            "Train Batch: 3000 Loss: 4.236950874328613\n",
            "Train Batch: 4000 Loss: 4.270349025726318\n",
            "Train Batch: 5000 Loss: 4.101980209350586\n",
            "Epoch 1 training time: 243.13 sec Training Loss: 4.259 , Valiation Loss: 4.147\n",
            "Train Batch: 0 Loss: 4.186620235443115\n",
            "Train Batch: 1000 Loss: 4.292304515838623\n",
            "Train Batch: 2000 Loss: 4.017712116241455\n",
            "Train Batch: 3000 Loss: 4.113926410675049\n",
            "Train Batch: 4000 Loss: 4.319486141204834\n",
            "Train Batch: 5000 Loss: 4.180914878845215\n",
            "Epoch 2 training time: 243.14 sec Training Loss: 4.153 , Valiation Loss: 4.082\n",
            "Train Batch: 0 Loss: 4.159313201904297\n",
            "Train Batch: 1000 Loss: 4.1537041664123535\n",
            "Train Batch: 2000 Loss: 4.131727695465088\n",
            "Train Batch: 3000 Loss: 3.957810878753662\n",
            "Train Batch: 4000 Loss: 3.9245364665985107\n",
            "Train Batch: 5000 Loss: 4.059909820556641\n",
            "Epoch 3 training time: 242.32 sec Training Loss: 4.093 , Valiation Loss: 4.046\n",
            "Train Batch: 0 Loss: 3.898347854614258\n",
            "Train Batch: 1000 Loss: 4.025694370269775\n",
            "Train Batch: 2000 Loss: 4.082043170928955\n",
            "Train Batch: 3000 Loss: 4.136032581329346\n",
            "Train Batch: 4000 Loss: 3.9222521781921387\n",
            "Train Batch: 5000 Loss: 4.071323394775391\n",
            "Epoch 4 training time: 242.05 sec Training Loss: 4.053 , Valiation Loss: 4.006\n",
            "Train Batch: 0 Loss: 3.908266305923462\n",
            "Train Batch: 1000 Loss: 4.176453590393066\n",
            "Train Batch: 2000 Loss: 4.126363277435303\n",
            "Train Batch: 3000 Loss: 4.233436584472656\n",
            "Train Batch: 4000 Loss: 3.7146902084350586\n",
            "Train Batch: 5000 Loss: 3.93013596534729\n",
            "Epoch 5 training time: 241.66 sec Training Loss: 4.024 , Valiation Loss: 3.992\n",
            "Train Batch: 0 Loss: 4.126580238342285\n",
            "Train Batch: 1000 Loss: 3.8135275840759277\n",
            "Train Batch: 2000 Loss: 4.04197883605957\n",
            "Train Batch: 3000 Loss: 3.9152731895446777\n",
            "Train Batch: 4000 Loss: 3.9575612545013428\n",
            "Train Batch: 5000 Loss: 4.05311393737793\n",
            "Epoch 6 training time: 241.59 sec Training Loss: 4.001 , Valiation Loss: 3.980\n",
            "Train Batch: 0 Loss: 3.903589963912964\n",
            "Train Batch: 1000 Loss: 3.843937873840332\n",
            "Train Batch: 2000 Loss: 4.237982273101807\n",
            "Train Batch: 3000 Loss: 3.905946731567383\n",
            "Train Batch: 4000 Loss: 3.8747310638427734\n",
            "Train Batch: 5000 Loss: 4.130029678344727\n",
            "Epoch 7 training time: 241.57 sec Training Loss: 3.983 , Valiation Loss: 3.965\n",
            "Train Batch: 0 Loss: 3.893521547317505\n",
            "Train Batch: 1000 Loss: 4.145390033721924\n",
            "Train Batch: 2000 Loss: 4.09847354888916\n",
            "Train Batch: 3000 Loss: 3.806058168411255\n",
            "Train Batch: 4000 Loss: 4.074019432067871\n",
            "Train Batch: 5000 Loss: 3.891902208328247\n",
            "Epoch 8 training time: 241.53 sec Training Loss: 3.968 , Valiation Loss: 3.953\n",
            "Train Batch: 0 Loss: 3.7048180103302\n",
            "Train Batch: 1000 Loss: 3.769833564758301\n",
            "Train Batch: 2000 Loss: 4.01058292388916\n",
            "Train Batch: 3000 Loss: 4.036447048187256\n",
            "Train Batch: 4000 Loss: 3.904789924621582\n",
            "Train Batch: 5000 Loss: 3.96907377243042\n",
            "Epoch 9 training time: 241.57 sec Training Loss: 3.956 , Valiation Loss: 3.945\n",
            "Train Batch: 0 Loss: 3.940213918685913\n",
            "Train Batch: 1000 Loss: 3.9040474891662598\n",
            "Train Batch: 2000 Loss: 3.8415493965148926\n",
            "Train Batch: 3000 Loss: 3.98288631439209\n",
            "Train Batch: 4000 Loss: 4.060533046722412\n",
            "Train Batch: 5000 Loss: 3.926478862762451\n",
            "Epoch 10 training time: 241.52 sec Training Loss: 3.944 , Valiation Loss: 3.939\n",
            "Train Batch: 0 Loss: 3.7883098125457764\n",
            "Train Batch: 1000 Loss: 3.6726338863372803\n",
            "Train Batch: 2000 Loss: 3.8920161724090576\n",
            "Train Batch: 3000 Loss: 3.9912476539611816\n",
            "Train Batch: 4000 Loss: 4.003721237182617\n",
            "Train Batch: 5000 Loss: 3.8701305389404297\n",
            "Epoch 11 training time: 241.76 sec Training Loss: 3.935 , Valiation Loss: 3.932\n",
            "Train Batch: 0 Loss: 4.000260829925537\n",
            "Train Batch: 1000 Loss: 3.7937047481536865\n",
            "Train Batch: 2000 Loss: 3.7456772327423096\n",
            "Train Batch: 3000 Loss: 3.9860730171203613\n",
            "Train Batch: 4000 Loss: 4.1382317543029785\n",
            "Train Batch: 5000 Loss: 4.083171844482422\n",
            "Epoch 12 training time: 241.74 sec Training Loss: 3.926 , Valiation Loss: 3.928\n",
            "Train Batch: 0 Loss: 3.908123254776001\n",
            "Train Batch: 1000 Loss: 3.8139562606811523\n",
            "Train Batch: 2000 Loss: 3.8640599250793457\n",
            "Train Batch: 3000 Loss: 4.050147533416748\n",
            "Train Batch: 4000 Loss: 4.039569854736328\n",
            "Train Batch: 5000 Loss: 4.043632984161377\n",
            "Epoch 13 training time: 241.86 sec Training Loss: 3.918 , Valiation Loss: 3.928\n",
            "Train Batch: 0 Loss: 3.8326385021209717\n",
            "Train Batch: 1000 Loss: 4.039790630340576\n",
            "Train Batch: 2000 Loss: 4.0280327796936035\n",
            "Train Batch: 3000 Loss: 3.9894449710845947\n",
            "Train Batch: 4000 Loss: 4.037845134735107\n",
            "Train Batch: 5000 Loss: 3.9691250324249268\n",
            "Epoch 14 training time: 241.65 sec Training Loss: 3.911 , Valiation Loss: 3.921\n",
            "Train Batch: 0 Loss: 3.7279911041259766\n",
            "Train Batch: 1000 Loss: 4.020472526550293\n",
            "Train Batch: 2000 Loss: 3.773430824279785\n",
            "Train Batch: 3000 Loss: 3.9930813312530518\n",
            "Train Batch: 4000 Loss: 4.004299640655518\n",
            "Train Batch: 5000 Loss: 4.01901388168335\n",
            "Epoch 15 training time: 240.88 sec Training Loss: 3.904 , Valiation Loss: 3.915\n",
            "Train Batch: 0 Loss: 3.97087025642395\n",
            "Train Batch: 1000 Loss: 4.0798187255859375\n",
            "Train Batch: 2000 Loss: 3.8015494346618652\n",
            "Train Batch: 3000 Loss: 3.7676360607147217\n",
            "Train Batch: 4000 Loss: 3.728729009628296\n",
            "Train Batch: 5000 Loss: 3.819845199584961\n",
            "Epoch 16 training time: 241.08 sec Training Loss: 3.898 , Valiation Loss: 3.914\n",
            "Train Batch: 0 Loss: 3.6865904331207275\n",
            "Train Batch: 1000 Loss: 3.9672815799713135\n",
            "Train Batch: 2000 Loss: 3.6572940349578857\n",
            "Train Batch: 3000 Loss: 4.027576923370361\n",
            "Train Batch: 4000 Loss: 3.8922791481018066\n",
            "Train Batch: 5000 Loss: 3.8731489181518555\n",
            "Epoch 17 training time: 240.90 sec Training Loss: 3.893 , Valiation Loss: 3.909\n",
            "Train Batch: 0 Loss: 3.729945421218872\n",
            "Train Batch: 1000 Loss: 3.7628424167633057\n",
            "Train Batch: 2000 Loss: 4.009894371032715\n",
            "Train Batch: 3000 Loss: 3.9045815467834473\n",
            "Train Batch: 4000 Loss: 3.8883941173553467\n",
            "Train Batch: 5000 Loss: 3.9909279346466064\n",
            "Epoch 18 training time: 240.69 sec Training Loss: 3.888 , Valiation Loss: 3.908\n",
            "Train Batch: 0 Loss: 3.9466552734375\n",
            "Train Batch: 1000 Loss: 3.7335660457611084\n",
            "Train Batch: 2000 Loss: 3.8659422397613525\n",
            "Train Batch: 3000 Loss: 3.9241926670074463\n",
            "Train Batch: 4000 Loss: 3.8902242183685303\n",
            "Train Batch: 5000 Loss: 3.744418144226074\n",
            "Epoch 19 training time: 241.36 sec Training Loss: 3.883 , Valiation Loss: 3.914\n",
            "Train Batch: 0 Loss: 3.9845614433288574\n",
            "Train Batch: 1000 Loss: 3.548757553100586\n",
            "Train Batch: 2000 Loss: 3.7435872554779053\n",
            "Train Batch: 3000 Loss: 4.058689594268799\n",
            "Train Batch: 4000 Loss: 3.999687910079956\n",
            "Train Batch: 5000 Loss: 3.9632418155670166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d18c20e02f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-89f22b614f6e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36mpool\u001b[0;34m(data, batch_size, key, batch_size_fn, random_shuffler, shuffle, sort_within_batch)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrandom_shuffler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mrandom_shuffler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(data, batch_size, batch_size_fn)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0msize_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_so_far\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_so_far\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePu_d6fSbLkZ"
      },
      "source": [
        "# 如果要保留訓練出來的模型，建議和 vocabulary 一起儲存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGtaydsuHQ2X"
      },
      "source": [
        "torch.save(CMN_FIELD.vocab, model_dir + 'vocab.pt')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC5fZzIUeByb"
      },
      "source": [
        "# 讀取訓練最佳結果\n",
        "-- 如果下載我們的訓練結果 別忘了讀取 vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLq4YhL1bFa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd0ca44-312e-43dd-8cd6-6d58294ce4b0"
      },
      "source": [
        "# 保留讀取之前儲存的 vocabulary\n",
        "CMN_FIELD.vocab = torch.load( '/content/drive/MyDrive/NLPMarathon/transformer/model/vocab.pt')\n",
        "TRG_FIELD.vocab = CMN_FIELD.vocab\n",
        "\n",
        "# model_dir =  '/content/drive/My Drive/cupoy/transformer/model/'\n",
        "# model.load_state_dict(torch.load( model_dir + 'model-ptt-best.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-8.pt'))\n",
        "test_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGJujTfe3b5"
      },
      "source": [
        "# 使用訓練結果產生回答\n",
        "- 用模型每一步最佳猜測產生回答"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sguk0pbPPT20"
      },
      "source": [
        "\n",
        "def simple_answer_ptt_question(sentence, qa_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [qa_field.init_token] + tokens + [\"<sep>\"]\n",
        "        \n",
        "    qa_indexes = [qa_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    qa_tensor = torch.LongTensor(qa_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "\n",
        "    for i in range(max_len):\n",
        "        qa_tensor = torch.LongTensor(qa_indexes).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            dec_qa, decoder_self_attention  = model(qa_tensor)\n",
        "        \n",
        "        pred_token = dec_qa.argmax(2)[:,-1].item()\n",
        "        qa_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == qa_field.vocab.stoi[qa_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    qa_tokens = [qa_field.vocab.itos[i] for i in qa_indexes]\n",
        "    answer = \"\".join(qa_tokens[qa_tokens.index(\"<sep>\")+1:-1])\n",
        "            \n",
        "    return answer,  decoder_self_attention"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrYey4fetvJ"
      },
      "source": [
        "# Fun Time\n",
        "-- 自己上 ptt 找新的標題來玩吧"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFx32UDKE9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be78674-27ae-45a0-f134-0c39360c2a9f"
      },
      "source": [
        "\n",
        "question = '把中國人惹翻了 會怎麼樣嗎？'\n",
        "\n",
        "qa_result, _ = simple_answer_ptt_question(question, CMN_FIELD, model, device, max_len = 50)\n",
        "\n",
        "print (qa_result)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中國人不會翻牆\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjnOca_aKgHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590ac5a8-3644-414b-c211-1d16c59a6004"
      },
      "source": [
        "\n",
        "question = '看到前女友生小孩是什麼感覺'\n",
        "\n",
        "qa_result, _ = simple_answer_ptt_question(question, CMN_FIELD, model, device, max_len = 50)\n",
        "\n",
        "print (qa_result)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "你是不是想幹人家\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dj0S_r_imAH",
        "outputId": "355671de-caf5-47e0-ee2e-2bba1198e0f9"
      },
      "source": [
        "question = '肥宅初夜可以賣多少？'\n",
        "\n",
        "qa_result, _ = simple_answer_ptt_question(question, CMN_FIELD, model, device, max_len = 50)\n",
        "\n",
        "print (qa_result)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "肥宅不要出門好嗎\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJUYzpMbizXl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}