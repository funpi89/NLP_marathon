{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "DitsilBert_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funpi89/NLP_marathon/blob/main/DitsilBert_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fqIqrSeIRiT",
        "outputId": "dfa7950f-cd01-4a61-ea4f-ec96dc7b3b10"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hilbwNOAXRQi"
      },
      "source": [
        "# DistilBert 預訓練模型的使用方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8DH1YzZXRQ1"
      },
      "source": [
        "- 資料來源 : https://www.kaggle.com/c/nlp-getting-started/overview\n",
        "判斷某一段推文是否企圖告訴我們\"天災\"正在發生,訓練集與要判斷的測試集都有數千筆資料，其中\"text”是推文內容，\"target\"是是否為天災"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRD9W_D9XT7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d3e54a-ed4a-4785-f6c3-d0ab3c445815"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l-gRDFdXRQ2"
      },
      "source": [
        "# 載入相關套件, 第一次執行前需安裝 transformers 套件\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re, warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLI23P4FXRQ3"
      },
      "source": [
        "# 載入訓練與測試資料\n",
        "df = pd.read_csv('/content/drive/MyDrive/NLPMarathon/huggingface/data/disaster/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/NLPMarathon/huggingface/data/disaster/test.csv') "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzYaEs1VXRQ4"
      },
      "source": [
        "# 前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4b-UBIrXRQ6"
      },
      "source": [
        "# 前處理-1 : 消除連字\n",
        "def decontracted(text):\n",
        "    # 特殊連字\n",
        "    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n",
        "    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n",
        "    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n",
        "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n",
        "    # 一般性連字\n",
        "    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n",
        "    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n",
        "    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: decontracted(x))\n",
        "df_test['text'] = df_test['text'].apply(lambda x: decontracted(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei8kZCfYXRQ7"
      },
      "source": [
        "# 前處理-2 : 清除特殊符號\n",
        "import string\n",
        "regular_punct = list(string.punctuation)\n",
        "extra_punct = [\n",
        "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
        "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
        "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
        "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
        "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
        "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
        "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
        "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
        "# 消除標點符號以及上列符號\n",
        "all_punct = list(set(regular_punct + extra_punct))\n",
        "# 消除連字號 \"-\" 以及句號 \".\"\n",
        "all_punct.remove('-')\n",
        "all_punct.remove('.')\n",
        "\n",
        "def spacing_punctuation(text):\n",
        "    \"\"\"\n",
        "    add space before and after punctuation and symbols\n",
        "    \"\"\"\n",
        "    for punc in all_punct:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, f' {punc} ')\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: spacing_punctuation(x))\n",
        "df_test['text'] = df_test['text'].apply(lambda x: spacing_punctuation(x))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R0vazyvXRRD"
      },
      "source": [
        "# 前處理-3 : 錯漏字修正\n",
        "mis_connect_list = ['(W|w)hat', '(W|w)hy', '(H|h)ow', '(W|w)hich', '(W|w)here', '(W|w)ill']\n",
        "mis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n",
        "\n",
        "mis_spell_mapping = {'whattsup': 'WhatsApp', 'whatasapp':'WhatsApp', 'whatsupp':'WhatsApp', \n",
        "                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n",
        "                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n",
        "                      # why\n",
        "                      'Whybis':'Why is', 'laowhy86':'Foreigners who do not respect China',\n",
        "                      'Whyco-education':'Why co-education',\n",
        "                      # How\n",
        "                      \"Howddo\":\"How do\", 'Howeber':'However', 'Showh':'Show',\n",
        "                      \"Willowmagic\":'Willow magic', 'WillsEye':'Will Eye', 'Williby':'will by'}\n",
        "def spacing_some_connect_words(text):\n",
        "    \"\"\"\n",
        "    'Whyare' -> 'Why are'\n",
        "    \"\"\"\n",
        "    ori = text\n",
        "    for error in mis_spell_mapping:\n",
        "        if error in text:\n",
        "            text = text.replace(error, mis_spell_mapping[error])\n",
        "            \n",
        "    # what\n",
        "    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
        "    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
        "    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
        "    # why\n",
        "    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
        "    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
        "    # How\n",
        "    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
        "    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
        "    # which\n",
        "    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
        "    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
        "    # where\n",
        "    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
        "    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
        "    \n",
        "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
        "    text = text.replace(\"What sApp\", 'WhatsApp') \n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: spacing_some_connect_words(x))\n",
        "df_test['text'] = df_test['text'].apply(lambda x: spacing_some_connect_words(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeABpGZ6JS3Y"
      },
      "source": [
        "# 調整訓練資料的大小(colab跑不完全部的量)\n",
        "df = df[:4000]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf0lSqXkXRRD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f4c48533-3571-4ef6-85e6-e29573146e0f"
      },
      "source": [
        "df.head() "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this  # earthquake...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to  ' shelter in place '  ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13 , 000 people receive  # wildfires evacuatio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby  # Alaska a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this  # earthquake...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to  ' shelter in place '  ...      1\n",
              "3   6     NaN  ...  13 , 000 people receive  # wildfires evacuatio...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby  # Alaska a...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bitA8KgQH-4A",
        "outputId": "f7e5003d-d8cb-4c05-9777-9d7201cbf74f"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about  # earthquake is different cities ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond ,  geese a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting.  # Spokane  # wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about  # earthquake is different cities ...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond ,  geese a...\n",
              "3   9     NaN      NaN       Apocalypse lighting.  # Spokane  # wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU3jE24GXRRF"
      },
      "source": [
        "# 載入 distilBERT 模型或 Bert 模型, 將文字編碼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAlhyqNLXRRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64eacf2f-6c79-4d7a-9bcd-5ec461a24b24"
      },
      "source": [
        "# 載入 distilBERT 模型\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "# 載入預訓練權重以及 tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "model = model.cuda()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REKqpxlsXRRH"
      },
      "source": [
        "# 將訓練資料經由 distilBERT 或 Bert 轉換為 Embedding 編碼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jk6RcUzXRRH"
      },
      "source": [
        "# 將訓練資料經過 tokenizer 編碼轉換\n",
        "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rta28pWPXRRU"
      },
      "source": [
        "# 以最長字串為準, 將訓練資料補零成相同長度\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsAA4aMmXRRV"
      },
      "source": [
        "# 設定 attention_mask, 將計算經過 Bert 生成的 Embedding 結果, 儲存於 last_hidden_states 中\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "input_ids = torch.tensor(padded).to(torch.int64).cuda()\n",
        "attention_mask = torch.tensor(attention_mask).to(torch.int64).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUwtSCPUXRRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f748ff6c-d579-44c6-d08e-07e5c3ac8a65"
      },
      "source": [
        "# 準備下一階段要用的特徵 (上階段 Embedding 結果) 與目標值\n",
        "labels = df['target']\n",
        "features = last_hidden_states[0][:,0,:].cpu().numpy()\n",
        "features[0].shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJNdmdv0XRRZ"
      },
      "source": [
        "# 切割訓練 / 測試集\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir5sKi3yXRRa"
      },
      "source": [
        "# 使用 Logistic Regression 當作最後一層, 輸出預測結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_l46zqRXRRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a4feca-67c7-4783-bd2c-432018b838e8"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
        "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
        "grid_search.fit(train_features, train_labels)\n",
        "print('best parameters: ', grid_search.best_params_)\n",
        "print('best scrores: ', grid_search.best_score_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best parameters:  {'C': 5.263252631578947}\n",
            "best scrores:  0.7953333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNXgd9ZvXRRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cd36af-3417-4003-dd18-08607a02cd5a"
      },
      "source": [
        "# 將上一格跑出的 Logistic Regression 最佳 C 值填入, 觀察測試集的驗證分數\n",
        "lr_clf = LogisticRegression(C = 5.263252631578947)  \n",
        "lr_clf.fit(train_features, train_labels)\n",
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvDnMohiXRRb"
      },
      "source": [
        "# 對預測目標資料做出最終預測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kIrjBk4XRRc"
      },
      "source": [
        "# 將預測目標資料經過 tokenizer 編碼轉換\n",
        "tokenized_t = df_test['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF23Ot9EXRRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d8b6e7-9b35-46d9-d3ef-9a191c1c4fc3"
      },
      "source": [
        "# 以最長字串為準, 將預測目標資料補零成相同長度\n",
        "max_len = 0\n",
        "for i in tokenized_t.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "        \n",
        "padded_t = np.array([i + [0]*(max_len-len(i)) for i in tokenized_t.values])\n",
        "np.array(padded_t).shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3263, 73)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUqEb0tiXRRd"
      },
      "source": [
        "# 設定 attention_mask, 將計算經過 Bert 生成的 Embedding 結果, 儲存於 last_hidden_states 中\n",
        "attention_mask_t = np.where(padded_t != 0, 1, 0)\n",
        "input_ids = torch.tensor(padded_t).to(torch.int64).cuda()\n",
        "attention_mask_t = torch.tensor(attention_mask_t).to(torch.int64).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask_t)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ospfTVoWXRRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b597b2-015e-473c-bafd-bdc962d78ff9"
      },
      "source": [
        "# 輸出預測目標資料的預測結果\n",
        "val_features = last_hidden_states[0][:,0,:].cpu().numpy() \n",
        "y_pred = lr_clf.predict(val_features)\n",
        "y_pred"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZYOkvjBP4lI"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO1Nr7gvXRRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81d9497c-6e84-4381-fc3d-9a43cd97f105"
      },
      "source": [
        "s = df_test['text'][15]\n",
        "s"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham is Wholesale Market http :  /  / t.co / irWqCEZWEU'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBIuLdCPM1fO",
        "outputId": "e9b78ca9-7840-4d8d-c725-d078d2665ad9"
      },
      "source": [
        "s_tokened = tokenizer.encode(s, add_special_tokens=True)\n",
        "\n",
        "padded = np.array(s_tokened + [0]*(max_len-len(s_tokened)))\n",
        "\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "input_ids = torch.tensor(padded).to(torch.int64).cuda()\n",
        "input_ids = torch.unsqueeze(input_ids, 0)\n",
        "attention_mask = torch.tensor(attention_mask).to(torch.int64).cuda()\n",
        "attention_mask = torch.unsqueeze(attention_mask, 0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "features = last_hidden_states[0][:,0,:].cpu().numpy()\n",
        "print(features.shape) \n",
        "pred = lr_clf.predict(features)\n",
        "pred\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 768)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1k8wD8sODz5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}